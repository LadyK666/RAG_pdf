import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import argparse
from pdfminer.high_level import extract_text_to_fp
from sentence_transformers import SentenceTransformer
from sentence_transformers import CrossEncoder
import faiss
import requests
import json
from io import StringIO
from langchain_text_splitters import RecursiveCharacterTextSplitter
import socket
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time
from datetime import datetime
import hashlib
import re
from dotenv import load_dotenv
from rank_bm25 import BM25Okapi
import numpy as np
import jieba
import threading
from functools import lru_cache

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
SERPAPI_KEY = os.getenv("SERPAPI_KEY")
SEARCH_ENGINE = "google"
RERANK_METHOD = os.getenv("RERANK_METHOD", "cross_encoder")
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
SILICONFLOW_API_KEY = ""
SILICONFLOW_API_URL = os.getenv("SILICONFLOW_API_URL", "https://api.siliconflow.cn/v1/chat/completions")

# é…ç½®è¯·æ±‚é‡è¯•
requests.adapters.DEFAULT_RETRIES = 3
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['NO_PROXY'] = 'localhost,127.0.0.1'

# åˆå§‹åŒ–ç»„ä»¶
EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')

# FAISSç›¸å…³å…¨å±€å˜é‡
faiss_index = None
faiss_contents_map = {}  # original_id -> content
faiss_metadatas_map = {} # original_id -> metadata
faiss_id_order_for_index = []

# åˆå§‹åŒ–äº¤å‰ç¼–ç å™¨
cross_encoder = None
cross_encoder_lock = threading.Lock()

class BM25IndexManager:
    def __init__(self):
        self.bm25_index = None
        self.doc_mapping = {}  # æ˜ å°„BM25ç´¢å¼•ä½ç½®åˆ°æ–‡æ¡£ID
        self.tokenized_corpus = []
        self.raw_corpus = []
        
    def build_index(self, documents, doc_ids):
        """æ„å»ºBM25ç´¢å¼•"""
        self.raw_corpus = documents
        self.doc_mapping = {i: doc_id for i, doc_id in enumerate(doc_ids)}
        
        # å¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯ï¼Œä½¿ç”¨jiebaåˆ†è¯å™¨æ›´é€‚åˆä¸­æ–‡
        self.tokenized_corpus = []
        for doc in documents:
            # å¯¹ä¸­æ–‡æ–‡æ¡£è¿›è¡Œåˆ†è¯
            tokens = list(jieba.cut(doc))
            self.tokenized_corpus.append(tokens)
        
        # åˆ›å»ºBM25ç´¢å¼•
        self.bm25_index = BM25Okapi(self.tokenized_corpus)
        return True
        
    def search(self, query, top_k=5):
        """ä½¿ç”¨BM25æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.bm25_index:
            return []
        
        # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
        tokenized_query = list(jieba.cut(query))
        
        # è·å–BM25å¾—åˆ†
        bm25_scores = self.bm25_index.get_scores(tokenized_query)
        
        # è·å–å¾—åˆ†æœ€é«˜çš„æ–‡æ¡£ç´¢å¼•
        top_indices = np.argsort(bm25_scores)[-top_k:][::-1]
        
        # è¿”å›ç»“æœ
        results = []
        for idx in top_indices:
            if bm25_scores[idx] > 0:  # åªè¿”å›æœ‰ç›¸å…³æ€§çš„ç»“æœ
                results.append({
                    'id': self.doc_mapping[idx],
                    'score': float(bm25_scores[idx]),
                    'content': self.raw_corpus[idx]
                })
        
        return results
    
    def clear(self):
        """æ¸…ç©ºç´¢å¼•"""
        self.bm25_index = None
        self.doc_mapping = {}
        self.tokenized_corpus = []
        self.raw_corpus = []

# åˆå§‹åŒ–BM25ç´¢å¼•ç®¡ç†å™¨
BM25_MANAGER = BM25IndexManager()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def get_cross_encoder():
    """å»¶è¿ŸåŠ è½½äº¤å‰ç¼–ç å™¨æ¨¡å‹"""
    global cross_encoder
    if cross_encoder is None:
        with cross_encoder_lock:
            if cross_encoder is None:
                try:
                    # ä½¿ç”¨å¤šè¯­è¨€äº¤å‰ç¼–ç å™¨ï¼Œæ›´é€‚åˆä¸­æ–‡
                    cross_encoder = CrossEncoder('sentence-transformers/distiluse-base-multilingual-cased-v2')
                    logging.info("äº¤å‰ç¼–ç å™¨åŠ è½½æˆåŠŸ")
                except Exception as e:
                    logging.error(f"åŠ è½½äº¤å‰ç¼–ç å™¨å¤±è´¥: {str(e)}")
                    # è®¾ç½®ä¸ºNoneï¼Œä¸‹æ¬¡è°ƒç”¨ä¼šé‡è¯•
                    cross_encoder = None
    return cross_encoder

def recursive_retrieval(initial_query, max_iterations=3, enable_web_search=False, model_choice="ollama"):
    """
    å®ç°é€’å½’æ£€ç´¢ä¸è¿­ä»£æŸ¥è¯¢åŠŸèƒ½
    é€šè¿‡åˆ†æå½“å‰æŸ¥è¯¢ç»“æœï¼Œç¡®å®šæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢
    
    Args:
        initial_query: åˆå§‹æŸ¥è¯¢
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        enable_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
        model_choice: ä½¿ç”¨çš„æ¨¡å‹é€‰æ‹©("ollama"æˆ–"siliconflow")
        
    Returns:
        åŒ…å«æ‰€æœ‰æ£€ç´¢å†…å®¹çš„åˆ—è¡¨
    """
    query = initial_query
    all_contexts = []
    all_doc_ids = []  # ä½¿ç”¨åŸå§‹ID
    all_metadata = []
    
    global faiss_index, faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index
    
    for i in range(max_iterations):
        logging.info(f"é€’å½’æ£€ç´¢è¿­ä»£ {i+1}/{max_iterations}ï¼Œå½“å‰æŸ¥è¯¢: {query}")
        
        web_results_texts = [] # Store text from web results for context building
        if enable_web_search and check_serpapi_key():
            try:
                # update_web_results now needs to handle FAISS directly or be adapted
                # For now, let's assume it returns texts to be added to context
                web_search_raw_results = update_web_results(query) # This function needs to be adapted for FAISS
                for res in web_search_raw_results:
                    text = f"æ ‡é¢˜ï¼š{res.get('title', '')}\\næ‘˜è¦ï¼š{res.get('snippet', '')}"
                    web_results_texts.append(text)
                    # We would also need to add these to faiss_index, faiss_contents_map etc.
                    # and get their FAISS indices if we want them to be part of semantic search.
                    # This part is complex due to dynamic addition and potential ID clashes.
                    # For now, web results are added as pure text context, not searched semantically *again* within this loop's FAISS query.
            except Exception as e:
                logging.error(f"ç½‘ç»œæœç´¢é”™è¯¯: {str(e)}")
        
        query_embedding = EMBED_MODEL.encode([query])
        query_embedding_np = np.array(query_embedding).astype('float32')
        
        semantic_results_docs = []
        semantic_results_metadatas = []
        semantic_results_ids = []

        if faiss_index and faiss_index.ntotal > 0:
            try:
                D, I = faiss_index.search(query_embedding_np, k=10) # D: distances, I: indices
                # I contains the internal FAISS indices. We need to map them back to original IDs.
                for faiss_idx in I[0]: # I[0] because query_embedding_np was a batch of 1
                    if faiss_idx != -1 and faiss_idx < len(faiss_id_order_for_index):
                        original_id = faiss_id_order_for_index[faiss_idx]
                        semantic_results_docs.append(faiss_contents_map.get(original_id, ""))
                        semantic_results_metadatas.append(faiss_metadatas_map.get(original_id, {}))
                        semantic_results_ids.append(original_id)
            except Exception as e:
                logging.error(f"FAISS æ£€ç´¢é”™è¯¯: {str(e)}")
        
        bm25_results = BM25_MANAGER.search(query, top_k=10) # BM25_MANAGER.search returns list of dicts
        
        # Adapt hybrid_merge to work with current data structures
        # It expects semantic_results in a specific format if we pass it directly
        # For now, prepare a structure similar to old semantic_results for hybrid_merge
        prepared_semantic_results_for_hybrid = {
            "ids": [semantic_results_ids],
            "documents": [semantic_results_docs],
            "metadatas": [semantic_results_metadatas]
        }

        hybrid_results = hybrid_merge(prepared_semantic_results_for_hybrid, bm25_results, alpha=0.7)
        
        doc_ids_current_iter = []
        docs_current_iter = []
        metadata_list_current_iter = []
        
        if hybrid_results:
            for doc_id, result_data in hybrid_results[:10]: # doc_id here is the original_id
                doc_ids_current_iter.append(doc_id)
                docs_current_iter.append(result_data['content'])
                metadata_list_current_iter.append(result_data['metadata'])
        
        if docs_current_iter:
            try:
                reranked_results = rerank_results(query, docs_current_iter, doc_ids_current_iter, metadata_list_current_iter, top_k=5)
            except Exception as e:
                logging.error(f"é‡æ’åºé”™è¯¯: {str(e)}")
                reranked_results = [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0}) 
                                  for doc_id, doc, meta in zip(doc_ids_current_iter, docs_current_iter, metadata_list_current_iter)]
        else:
            reranked_results = []
        
        current_contexts_for_llm = web_results_texts[:] # Start with web results for LLM context
        for doc_id, result_data in reranked_results:
            doc = result_data['content']
            metadata = result_data['metadata']
            
            if doc_id not in all_doc_ids:  
                all_doc_ids.append(doc_id)
                all_contexts.append(doc)
                all_metadata.append(metadata)
            current_contexts_for_llm.append(doc) # Add reranked local docs for LLM context
        
        if i == max_iterations - 1:
            break
            
        if current_contexts_for_llm: # Use combined web and local context for deciding next query
            current_summary = "\\n".join(current_contexts_for_llm[:3]) if current_contexts_for_llm else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
            
            next_query_prompt = f"""åŸºäºåŸå§‹é—®é¢˜: {initial_query}
ä»¥åŠå·²æ£€ç´¢ä¿¡æ¯: 
{current_summary}

åˆ†ææ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢ã€‚å¦‚æœéœ€è¦ï¼Œè¯·æä¾›æ–°çš„æŸ¥è¯¢é—®é¢˜ï¼Œä½¿ç”¨ä¸åŒè§’åº¦æˆ–æ›´å…·ä½“çš„å…³é”®è¯ã€‚
å¦‚æœå·²ç»æœ‰å……åˆ†ä¿¡æ¯ï¼Œè¯·å›å¤'ä¸éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢'ã€‚

æ–°æŸ¥è¯¢(å¦‚æœéœ€è¦):"""
            
            try:
                if model_choice == "siliconflow":
                    logging.info("ä½¿ç”¨SiliconFlow APIåˆ†ææ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢")
                    next_query_result = call_siliconflow_api(next_query_prompt, temperature=0.7, max_tokens=256)
                    # SiliconFlow APIè¿”å›æ ¼å¼åŒ…å«å›ç­”å’Œå¯èƒ½çš„æ€ç»´é“¾ï¼Œè¿™é‡Œåªéœ€è¦å›ç­”éƒ¨åˆ†æ¥åˆ¤æ–­æ˜¯å¦ç»§ç»­
                    # å‡è®¾call_siliconflow_apiè¿”å›çš„æ˜¯ä¸€ä¸ªå…ƒç»„ (å›ç­”, æ€ç»´é“¾) æˆ–åªæ˜¯å›ç­”å­—ç¬¦ä¸²
                    if isinstance(next_query_result, tuple):
                         next_query = next_query_result[0].strip() # å–å›ç­”éƒ¨åˆ†
                    else:
                         next_query = next_query_result.strip() # å¦‚æœåªè¿”å›å­—ç¬¦ä¸²

                    # ç§»é™¤æ½œåœ¨çš„æ€ç»´é“¾æ ‡è®°
                    if "<think>" in next_query:
                        next_query = next_query.split("<think>")[0].strip()


                else:
                    logging.info("ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹åˆ†ææ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢")
                    response = session.post(
                        "http://localhost:11434/api/generate",
                        json={
                            "model": "deepseek-r1:1.5b",
                            "prompt": next_query_prompt,
                            "stream": False
                        },
                        timeout=30
                    )
                    # Ollama è¿”å›æ ¼å¼ä¸åŒï¼Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µæå–
                    next_query = response.json().get("response", "").strip()
                
                if "ä¸éœ€è¦" in next_query or "ä¸éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢" in next_query or len(next_query) < 5:
                    logging.info("LLMåˆ¤æ–­ä¸éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢ï¼Œç»“æŸé€’å½’æ£€ç´¢")
                    break
                    
                # ä½¿ç”¨æ–°æŸ¥è¯¢ç»§ç»­è¿­ä»£
                query = next_query
                logging.info(f"ç”Ÿæˆæ–°æŸ¥è¯¢: {query}")
            except Exception as e:
                logging.error(f"ç”Ÿæˆæ–°æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}")
                break
        else:
            # å¦‚æœå½“å‰è¿­ä»£æ²¡æœ‰æ£€ç´¢åˆ°å†…å®¹ï¼Œç»“æŸè¿­ä»£
            break
    
    return all_contexts, all_doc_ids, all_metadata

def update_web_results(query: str, num_results: int = 5) -> list:
    """
    åŸºäº SerpAPI æœç´¢ç»“æœã€‚æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬ä¸å°†ç»“æœå­˜å…¥FAISSã€‚
    å®ƒä»…è¿”å›åŸå§‹æœç´¢ç»“æœã€‚
    """
    results = serpapi_search(query, num_results)
    if not results:
        logging.info("ç½‘ç»œæœç´¢æ²¡æœ‰è¿”å›ç»“æœæˆ–å‘ç”Ÿé”™è¯¯")
        return []
    
    # ä¹‹å‰è¿™é‡Œæœ‰åˆ é™¤æ—§ç½‘ç»œç»“æœå’Œæ·»åŠ åˆ°ChromaDBçš„é€»è¾‘ã€‚
    # ç”±äºFAISS IndexFlatL2ä¸æ”¯æŒæŒ‰IDåˆ é™¤ï¼Œå¹¶ä¸”åŠ¨æ€æ·»åŠ æ¶‰åŠå¤æ‚IDç®¡ç†ï¼Œ
    # æ­¤ç®€åŒ–ç‰ˆæœ¬ä¸å°†ç½‘ç»œç»“æœæ·»åŠ åˆ°FAISSç´¢å¼•ã€‚
    # è¿”å›åŸå§‹ç»“æœï¼Œä¾›è°ƒç”¨è€…å†³å®šå¦‚ä½•ä½¿ç”¨ï¼ˆä¾‹å¦‚ï¼Œä»…ä½œä¸ºæ–‡æœ¬ä¸Šä¸‹æ–‡ï¼‰ã€‚
    logging.info(f"ç½‘ç»œæœç´¢è¿”å› {len(results)} æ¡ç»“æœï¼Œè¿™äº›ç»“æœä¸ä¼šè¢«æ·»åŠ åˆ°FAISSç´¢å¼•ä¸­ã€‚")
    return results # è¿”å›åŸå§‹SerpAPIç»“æœåˆ—è¡¨

# æ£€æŸ¥æ˜¯å¦é…ç½®äº†SERPAPI_KEY
def check_serpapi_key():
    """æ£€æŸ¥æ˜¯å¦é…ç½®äº†SERPAPI_KEY"""
    return SERPAPI_KEY is not None and SERPAPI_KEY.strip() != ""

# æ·»åŠ æ–‡ä»¶å¤„ç†çŠ¶æ€è·Ÿè¸ª
class FileProcessor:
    def __init__(self):
        self.processed_files = {}  # å­˜å‚¨å·²å¤„ç†æ–‡ä»¶çš„çŠ¶æ€
        
    def clear_files(self):
        """æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶è®°å½•"""
        self.processed_files = {}
        
    def add_file(self, file_name):
        self.processed_files[file_name] = {
            'status': 'ç­‰å¾…å¤„ç†',
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'chunks': 0
        }
        
    def update_status(self, file_name, status, chunks=None):
        if file_name in self.processed_files:
            self.processed_files[file_name]['status'] = status
            if chunks is not None:
                self.processed_files[file_name]['chunks'] = chunks
                
    def get_file_list(self):
        return [
            f"ğŸ“„ {fname} | {info['status']}"
            for fname, info in self.processed_files.items()
        ]

file_processor = FileProcessor()

#########################################
# çŸ›ç›¾æ£€æµ‹å‡½æ•°
#########################################
def detect_conflicts(sources):
    """ç²¾å‡†çŸ›ç›¾æ£€æµ‹ç®—æ³•"""
    key_facts = {}
    for item in sources:
        facts = extract_facts(item['text'] if 'text' in item else item.get('excerpt', ''))
        for fact, value in facts.items():
            if fact in key_facts:
                if key_facts[fact] != value:
                    return True
            else:
                key_facts[fact] = value
    return False

def extract_facts(text):
    """ä»æ–‡æœ¬æå–å…³é”®äº‹å®ï¼ˆç¤ºä¾‹é€»è¾‘ï¼‰"""
    facts = {}
    # æå–æ•°å€¼å‹äº‹å®
    numbers = re.findall(r'\b\d{4}å¹´|\b\d+%', text)
    if numbers:
        facts['å…³é”®æ•°å€¼'] = numbers
    # æå–æŠ€æœ¯æœ¯è¯­
    if "äº§ä¸šå›¾è°±" in text:
        facts['æŠ€æœ¯æ–¹æ³•'] = list(set(re.findall(r'[A-Za-z]+æ¨¡å‹|[A-Z]{2,}ç®—æ³•', text)))
    return facts

def evaluate_source_credibility(source):
    """è¯„ä¼°æ¥æºå¯ä¿¡åº¦"""
    credibility_scores = {
        "gov.cn": 0.9,
        "edu.cn": 0.85,
        "weixin": 0.7,
        "zhihu": 0.6,
        "baidu": 0.5
    }
    
    url = source.get('url', '')
    if not url:
        return 0.5  # é»˜è®¤ä¸­ç­‰å¯ä¿¡åº¦
    
    domain_match = re.search(r'//([^/]+)', url)
    if not domain_match:
        return 0.5
    
    domain = domain_match.group(1)
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•å·²çŸ¥åŸŸå
    for known_domain, score in credibility_scores.items():
        if known_domain in domain:
            return score
    
    return 0.5  # é»˜è®¤ä¸­ç­‰å¯ä¿¡åº¦

def extract_text(filepath):
    """æ”¹è¿›çš„PDFæ–‡æœ¬æå–æ–¹æ³•"""
    output = StringIO()
    with open(filepath, 'rb') as file:
        extract_text_to_fp(file, output)
    return output.getvalue()


from pathlib import Path
def process_multiple_pdfs(files):
    """å¤„ç†å¤šä¸ªPDFæ–‡ä»¶"""
    if not files:
        return "è¯·é€‰æ‹©è¦ä¸Šä¼ çš„PDFæ–‡ä»¶", []
    
    try:
        # æ¸…ç©ºå‘é‡æ•°æ®åº“å’Œç›¸å…³å­˜å‚¨
        # progress(0.1, desc="æ¸…ç†å†å²æ•°æ®...")
        global faiss_index, faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index
        faiss_index = None
        faiss_contents_map = {}
        faiss_metadatas_map = {}
        faiss_id_order_for_index = []
        
        # æ¸…ç©ºBM25ç´¢å¼•
        BM25_MANAGER.clear()
        logging.info("æˆåŠŸæ¸…ç†å†å²FAISSæ•°æ®å’ŒBM25ç´¢å¼•")
        
        # æ¸…ç©ºæ–‡ä»¶å¤„ç†çŠ¶æ€
        file_processor.clear_files()
        
        total_files = len(files)
        processed_results = []
        total_chunks = 0
        
        all_new_chunks = []
        all_new_metadatas = []
        all_new_original_ids = []
        
        for idx, file in enumerate(files, 1):
            pdf_path = Path(file)
            file_name = pdf_path.name
            try:
                # file_name = os.path.basename(file.name)
                # progress((idx-1)/total_files, desc=f"å¤„ç†æ–‡ä»¶ {idx}/{total_files}: {file_name}")
                
                file_processor.add_file(file_name)
                text = extract_text(str(pdf_path))
                
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=400,        
                    chunk_overlap=40,     
                    separators=["\n\n", "\n", "ã€‚", "ï¼Œ", "ï¼›", "ï¼š", " ", ""]
                )
                chunks = text_splitter.split_text(text)
                
                if not chunks:
                    raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬")
                
                doc_id = f"doc_{int(time.time())}_{idx}"
                
                # Store chunks and metadatas temporarily before batch embedding
                current_file_ids = [f"{doc_id}_chunk_{i}" for i in range(len(chunks))]
                current_file_metadatas = [{"source": file_name, "doc_id": doc_id} for _ in chunks]

                all_new_chunks.extend(chunks)
                all_new_metadatas.extend(current_file_metadatas)
                all_new_original_ids.extend(current_file_ids)
                
                total_chunks += len(chunks)
                file_processor.update_status(file_name, "å¤„ç†å®Œæˆ", len(chunks))
                processed_results.append(f"âœ… {file_name}: æˆåŠŸå¤„ç† {len(chunks)} ä¸ªæ–‡æœ¬å—")
                
            except Exception as e:
                error_msg = str(e)
                logging.error(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {error_msg}")
                file_processor.update_status(file_name, f"å¤„ç†å¤±è´¥: {error_msg}")
                processed_results.append(f"âŒ {file_name}: å¤„ç†å¤±è´¥ - {error_msg}")
        
        if all_new_chunks:
            # progress(0.8, desc="ç”Ÿæˆæ–‡æœ¬åµŒå…¥...")
            embeddings = EMBED_MODEL.encode(all_new_chunks, show_progress_bar=True)
            embeddings_np = np.array(embeddings).astype('float32')
            
            # progress(0.9, desc="æ„å»ºFAISSç´¢å¼•...")
            if faiss_index is None: # Should always be None here due to clearing
                dimension = embeddings_np.shape[1]
                faiss_index = faiss.IndexFlatL2(dimension)
            
            faiss_index.add(embeddings_np)
            
            for i, original_id in enumerate(all_new_original_ids):
                faiss_contents_map[original_id] = all_new_chunks[i]
                faiss_metadatas_map[original_id] = all_new_metadatas[i]
            faiss_id_order_for_index.extend(all_new_original_ids) # Keep track of order for FAISS indices
            logging.info(f"FAISSç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±ç´¢å¼• {faiss_index.ntotal} ä¸ªæ–‡æœ¬å—")

        summary = f"\næ€»è®¡å¤„ç† {total_files} ä¸ªæ–‡ä»¶ï¼Œ{total_chunks} ä¸ªæ–‡æœ¬å—"
        processed_results.append(summary)
        
        # progress(0.95, desc="æ„å»ºBM25æ£€ç´¢ç´¢å¼•...")
        update_bm25_index() # This will need to use faiss_contents_map
        
        file_list = file_processor.get_file_list()
        
        return "\n".join(processed_results), file_list
        
    except Exception as e:
        error_msg = str(e)
        logging.error(f"æ•´ä½“å¤„ç†è¿‡ç¨‹å‡ºé”™: {error_msg}")
        return f"å¤„ç†è¿‡ç¨‹å‡ºé”™: {error_msg}", []

# æ–°å¢ï¼šäº¤å‰ç¼–ç å™¨é‡æ’åºå‡½æ•°
def rerank_with_cross_encoder(query, docs, doc_ids, metadata_list, top_k=5):
    """
    ä½¿ç”¨äº¤å‰ç¼–ç å™¨å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº
    
    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        docs: æ–‡æ¡£å†…å®¹åˆ—è¡¨
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        top_k: è¿”å›ç»“æœæ•°é‡
        
    è¿”å›:
        é‡æ’åºåçš„ç»“æœåˆ—è¡¨ [(doc_id, {'content': doc, 'metadata': metadata, 'score': score}), ...]
    """
    if not docs:
        return []
        
    encoder = get_cross_encoder()
    if encoder is None:
        logging.warning("äº¤å‰ç¼–ç å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡é‡æ’åº")
        # è¿”å›åŸå§‹é¡ºåºï¼ˆæŒ‰ç´¢å¼•æ’åºï¼‰
        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)}) 
                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]
    
    # å‡†å¤‡äº¤å‰ç¼–ç å™¨è¾“å…¥
    cross_inputs = [[query, doc] for doc in docs]
    
    try:
        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
        scores = encoder.predict(cross_inputs)
        
        # ç»„åˆç»“æœ
        results = [
            (doc_id, {
                'content': doc, 
                'metadata': meta,
                'score': float(score)  # ç¡®ä¿æ˜¯PythonåŸç”Ÿç±»å‹
            }) 
            for doc_id, doc, meta, score in zip(doc_ids, docs, metadata_list, scores)
        ]
        
        # æŒ‰å¾—åˆ†æ’åº
        results = sorted(results, key=lambda x: x[1]['score'], reverse=True)
        
        # è¿”å›å‰Kä¸ªç»“æœ
        return results[:top_k]
    except Exception as e:
        logging.error(f"äº¤å‰ç¼–ç å™¨é‡æ’åºå¤±è´¥: {str(e)}")
        # å‡ºé”™æ—¶è¿”å›åŸå§‹é¡ºåº
        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)}) 
                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]

# æ–°å¢ï¼šLLMç›¸å…³æ€§è¯„åˆ†å‡½æ•°
@lru_cache(maxsize=32)
def get_llm_relevance_score(query, doc):
    """
    ä½¿ç”¨LLMå¯¹æŸ¥è¯¢å’Œæ–‡æ¡£çš„ç›¸å…³æ€§è¿›è¡Œè¯„åˆ†ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        doc: æ–‡æ¡£å†…å®¹
        
    è¿”å›:
        ç›¸å…³æ€§å¾—åˆ† (0-10)
    """
    try:
        # æ„å»ºè¯„åˆ†æç¤ºè¯
        prompt = f"""ç»™å®šä»¥ä¸‹æŸ¥è¯¢å’Œæ–‡æ¡£ç‰‡æ®µï¼Œè¯„ä¼°å®ƒä»¬çš„ç›¸å…³æ€§ã€‚
        è¯„åˆ†æ ‡å‡†ï¼š0åˆ†è¡¨ç¤ºå®Œå…¨ä¸ç›¸å…³ï¼Œ10åˆ†è¡¨ç¤ºé«˜åº¦ç›¸å…³ã€‚
        åªéœ€è¿”å›ä¸€ä¸ª0-10ä¹‹é—´çš„æ•´æ•°åˆ†æ•°ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–è§£é‡Šã€‚
        
        æŸ¥è¯¢: {query}
        
        æ–‡æ¡£ç‰‡æ®µ: {doc}
        
        ç›¸å…³æ€§åˆ†æ•°(0-10):"""
        
        # è°ƒç”¨æœ¬åœ°LLM
        response = session.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "deepseek-r1:1.5b",  # ä½¿ç”¨è¾ƒå°æ¨¡å‹è¿›è¡Œè¯„åˆ†
                "prompt": prompt,
                "stream": False
            },
            timeout=30
        )
        
        # æå–å¾—åˆ†
        result = response.json().get("response", "").strip()
        
        # å°è¯•è§£æä¸ºæ•°å­—
        try:
            score = float(result)
            # ç¡®ä¿åˆ†æ•°åœ¨0-10èŒƒå›´å†…
            score = max(0, min(10, score))
            return score
        except ValueError:
            # å¦‚æœæ— æ³•è§£æä¸ºæ•°å­—ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å­—
            match = re.search(r'\b([0-9]|10)\b', result)
            if match:
                return float(match.group(1))
            else:
                # é»˜è®¤è¿”å›ä¸­ç­‰ç›¸å…³æ€§
                return 5.0
                
    except Exception as e:
        logging.error(f"LLMè¯„åˆ†å¤±è´¥: {str(e)}")
        # é»˜è®¤è¿”å›ä¸­ç­‰ç›¸å…³æ€§
        return 5.0

def rerank_with_llm(query, docs, doc_ids, metadata_list, top_k=5):
    """
    ä½¿ç”¨LLMå¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº
    
    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        docs: æ–‡æ¡£å†…å®¹åˆ—è¡¨
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        top_k: è¿”å›ç»“æœæ•°é‡
    
    è¿”å›:
        é‡æ’åºåçš„ç»“æœåˆ—è¡¨
    """
    if not docs:
        return []
    
    results = []
    
    # å¯¹æ¯ä¸ªæ–‡æ¡£è¿›è¡Œè¯„åˆ†
    for doc_id, doc, meta in zip(doc_ids, docs, metadata_list):
        # è·å–LLMè¯„åˆ†
        score = get_llm_relevance_score(query, doc)
        
        # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
        results.append((doc_id, {
            'content': doc, 
            'metadata': meta,
            'score': score / 10.0  # å½’ä¸€åŒ–åˆ°0-1
        }))
    
    # æŒ‰å¾—åˆ†æ’åº
    results = sorted(results, key=lambda x: x[1]['score'], reverse=True)
    
    # è¿”å›å‰Kä¸ªç»“æœ
    return results[:top_k]

# æ–°å¢ï¼šé€šç”¨é‡æ’åºå‡½æ•°
def rerank_results(query, docs, doc_ids, metadata_list, method=None, top_k=5):
    """
    å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº
    
    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        docs: æ–‡æ¡£å†…å®¹åˆ—è¡¨
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        method: é‡æ’åºæ–¹æ³• ("cross_encoder", "llm" æˆ– None)
        top_k: è¿”å›ç»“æœæ•°é‡
        
    è¿”å›:
        é‡æ’åºåçš„ç»“æœ
    """
    # å¦‚æœæœªæŒ‡å®šæ–¹æ³•ï¼Œä½¿ç”¨å…¨å±€é…ç½®
    if method is None:
        method = RERANK_METHOD
    
    # æ ¹æ®æ–¹æ³•é€‰æ‹©é‡æ’åºå‡½æ•°
    if method == "llm":
        return rerank_with_llm(query, docs, doc_ids, metadata_list, top_k)
    elif method == "cross_encoder":
        return rerank_with_cross_encoder(query, docs, doc_ids, metadata_list, top_k)
    else:
        # é»˜è®¤ä¸è¿›è¡Œé‡æ’åºï¼ŒæŒ‰åŸå§‹é¡ºåºè¿”å›
        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)}) 
                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]

def stream_answer(question, enable_web_search=False, model_choice="ollama"):
    """æ”¹è¿›çš„æµå¼é—®ç­”å¤„ç†æµç¨‹ï¼Œæ”¯æŒè”ç½‘æœç´¢ã€æ··åˆæ£€ç´¢å’Œé‡æ’åºï¼Œä»¥åŠå¤šç§æ¨¡å‹é€‰æ‹©"""
    global faiss_index # ç¡®ä¿å¯ä»¥è®¿é—®
    try:
        # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦ä¸ºç©º
        knowledge_base_exists = faiss_index is not None and faiss_index.ntotal > 0
        if not knowledge_base_exists:
                if not enable_web_search:
                    yield "âš ï¸ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚", "é‡åˆ°é”™è¯¯"
                    return
                else:
                    logging.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œå°†ä»…ä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœ")
        
        # progress(0.3, desc="æ‰§è¡Œé€’å½’æ£€ç´¢...")
        # ä½¿ç”¨é€’å½’æ£€ç´¢è·å–æ›´å…¨é¢çš„ç­”æ¡ˆä¸Šä¸‹æ–‡
        all_contexts, all_doc_ids, all_metadata = recursive_retrieval(
            initial_query=question,
            max_iterations=3,
            enable_web_search=enable_web_search,
            model_choice=model_choice
        )
        
        # ç»„åˆä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¥æºä¿¡æ¯
        context_with_sources = []
        sources_for_conflict_detection = []
        
        # ä½¿ç”¨æ£€ç´¢åˆ°çš„ç»“æœæ„å»ºä¸Šä¸‹æ–‡
        for doc, doc_id, metadata in zip(all_contexts, all_doc_ids, all_metadata):
            source_type = metadata.get('source', 'æœ¬åœ°æ–‡æ¡£')
            
            source_item = {
                'text': doc,
                'type': source_type
            }
            
            if source_type == 'web':
                url = metadata.get('url', 'æœªçŸ¥URL')
                title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                context_with_sources.append(f"[ç½‘ç»œæ¥æº: {title}] (URL: {url})\n{doc}")
                source_item['url'] = url
                source_item['title'] = title
            else:
                source = metadata.get('source', 'æœªçŸ¥æ¥æº')
                context_with_sources.append(f"[æœ¬åœ°æ–‡æ¡£: {source}]\n{doc}")
                source_item['source'] = source
            
            sources_for_conflict_detection.append(source_item)
        
        # æ£€æµ‹çŸ›ç›¾
        conflict_detected = detect_conflicts(sources_for_conflict_detection)
        
        # è·å–å¯ä¿¡æº
        if conflict_detected:
            credible_sources = [s for s in sources_for_conflict_detection 
                               if s['type'] == 'web' and evaluate_source_credibility(s) > 0.7]
        
        context = "\n\n".join(context_with_sources)
        
        # æ·»åŠ æ—¶é—´æ•æ„Ÿæ£€æµ‹
        time_sensitive = any(word in question for word in ["æœ€æ–°", "ä»Šå¹´", "å½“å‰", "æœ€è¿‘", "åˆšåˆš"])
        
        # æ”¹è¿›æç¤ºè¯æ¨¡æ¿ï¼Œæé«˜å›ç­”è´¨é‡
        prompt_template = """ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ï¼Œä½ éœ€è¦åŸºäºä»¥ä¸‹{context_type}å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æä¾›çš„å‚è€ƒå†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·éµå¾ªä»¥ä¸‹å›ç­”åŸåˆ™ï¼š
1. ä»…åŸºäºæä¾›çš„å‚è€ƒå†…å®¹å›ç­”é—®é¢˜ï¼Œä¸è¦ä½¿ç”¨ä½ è‡ªå·±çš„çŸ¥è¯†
2. å¦‚æœå‚è€ƒå†…å®¹ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·å¦è¯šå‘ŠçŸ¥ä½ æ— æ³•å›ç­”,å¦‚æœå­˜åœ¨ä½ è®¤ä¸ºå’Œé—®é¢˜ç­‰ä»·çš„æ¦‚å¿µå’Œä¿¡æ¯ï¼Œå¯ä»¥ä½œä¸ºç­”æ¡ˆå›ç­”
3. å›ç­”åº”è¯¥å…¨é¢ã€å‡†ç¡®ã€æœ‰æ¡ç†ï¼Œå¹¶ä½¿ç”¨é€‚å½“çš„æ®µè½å’Œç»“æ„
4. è¯·ç”¨ä¸­æ–‡å›ç­”
5. åœ¨å›ç­”æœ«å°¾æ ‡æ³¨ä¿¡æ¯æ¥æº{time_instruction}{conflict_instruction}

è¯·ç°åœ¨å¼€å§‹å›ç­”ï¼š"""
        
        prompt = prompt_template.format(
            context_type="æœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œæœç´¢ç»“æœ" if enable_web_search and knowledge_base_exists else ("ç½‘ç»œæœç´¢ç»“æœ" if enable_web_search else "æœ¬åœ°æ–‡æ¡£"),
            context=context if context else ("ç½‘ç»œæœç´¢ç»“æœå°†ç”¨äºå›ç­”ã€‚" if enable_web_search and not knowledge_base_exists else "çŸ¥è¯†åº“ä¸ºç©ºæˆ–æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"),
            question=question,
            time_instruction="ï¼Œä¼˜å…ˆä½¿ç”¨æœ€æ–°çš„ä¿¡æ¯" if time_sensitive and enable_web_search else "",
            conflict_instruction="ï¼Œå¹¶æ˜ç¡®æŒ‡å‡ºä¸åŒæ¥æºçš„å·®å¼‚" if conflict_detected else ""
        )
        
        # progress(0.7, desc="ç”Ÿæˆå›ç­”...")
        full_answer = ""
        
        # æ ¹æ®æ¨¡å‹é€‰æ‹©ä½¿ç”¨ä¸åŒçš„API
        if model_choice == "siliconflow":
            # å¯¹äºSiliconFlow APIï¼Œä¸æ”¯æŒæµå¼å“åº”ï¼Œæ‰€ä»¥ä¸€æ¬¡æ€§è·å–
            # progress(0.8, desc="é€šè¿‡SiliconFlow APIç”Ÿæˆå›ç­”...")
            full_answer = call_siliconflow_api(prompt, temperature=0.7, max_tokens=1536)
            
            # å¤„ç†æ€ç»´é“¾
            if "<think>" in full_answer and "</think>" in full_answer:
                processed_answer = process_thinking_content(full_answer)
            else:
                processed_answer = full_answer
                
            yield processed_answer, "å®Œæˆ!"
        else:
            # ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹çš„æµå¼å“åº”
            response = session.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "deepseek-r1:1.5b",
                    "prompt": prompt,
                    "stream": True
                },
                timeout=120,
                stream=True
            )
            
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line.decode()).get("response", "")
                    full_answer += chunk
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„æ€ç»´é“¾æ ‡ç­¾å¯ä»¥å¤„ç†
                    if "<think>" in full_answer and "</think>" in full_answer:
                        # éœ€è¦ç¡®ä¿å®Œæ•´æ”¶é›†ä¸€ä¸ªæ€ç»´é“¾ç‰‡æ®µåå†æ˜¾ç¤º
                        processed_answer = process_thinking_content(full_answer)
                    else:
                        processed_answer = full_answer
                    
                    yield processed_answer, "ç”Ÿæˆå›ç­”ä¸­..."
                    
            # å¤„ç†æœ€ç»ˆè¾“å‡ºï¼Œç¡®ä¿åº”ç”¨æ€ç»´é“¾å¤„ç†
            final_answer = process_thinking_content(full_answer)
            yield final_answer, "å®Œæˆ!"
        
    except Exception as e:
        yield f"ç³»ç»Ÿé”™è¯¯: {str(e)}", "é‡åˆ°é”™è¯¯"

def query_answer(question, enable_web_search=False, model_choice="ollama"):
    """é—®ç­”å¤„ç†æµç¨‹ï¼Œæ”¯æŒè”ç½‘æœç´¢ã€æ··åˆæ£€ç´¢å’Œé‡æ’åºï¼Œä»¥åŠå¤šç§æ¨¡å‹é€‰æ‹©"""
    global faiss_index
    try:
        logging.info(f"æ”¶åˆ°é—®é¢˜ï¼š{question}ï¼Œè”ç½‘çŠ¶æ€ï¼š{enable_web_search}ï¼Œæ¨¡å‹é€‰æ‹©ï¼š{model_choice}")
        
        # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦ä¸ºç©º
        knowledge_base_exists = faiss_index is not None and faiss_index.ntotal > 0
        if not knowledge_base_exists:
            if not enable_web_search:
                return "âš ï¸ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚"
            else:
                logging.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œå°†ä»…ä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœ")
        
        print("æ‰§è¡Œé€’å½’æ£€ç´¢...")
        # ä½¿ç”¨é€’å½’æ£€ç´¢è·å–æ›´å…¨é¢çš„ç­”æ¡ˆä¸Šä¸‹æ–‡
        all_contexts, all_doc_ids, all_metadata = recursive_retrieval(
            initial_query=question,
            max_iterations=3,
            enable_web_search=enable_web_search,
            model_choice=model_choice
        )
        
        # ç»„åˆä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¥æºä¿¡æ¯
        context_with_sources = []
        sources_for_conflict_detection = []
        
        for i, (context, doc_id, metadata) in enumerate(zip(all_contexts, all_doc_ids, all_metadata)):
            source_info = f"[æ¥æº {i+1}]: {metadata.get('source', 'æœªçŸ¥')}"
            context_with_sources.append(f"{source_info}\n{context}")
            sources_for_conflict_detection.append({
                'text': context,
                'source': metadata.get('source', 'æœªçŸ¥')
            })
        
        # æ£€æµ‹ä¿¡æ¯å†²çª
        has_conflicts = detect_conflicts(sources_for_conflict_detection)
        if has_conflicts:
            print("âš ï¸ æ£€æµ‹åˆ°ä¿¡æ¯å†²çªï¼Œè¯·è°¨æ…å‚è€ƒ")
        
        # æ„å»ºæç¤ºè¯
        context_text = "\n\n".join(context_with_sources)
        prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜æ— æ³•å›ç­”ã€‚

å‚è€ƒä¿¡æ¯ï¼š
{context_text}

é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼Œå¹¶åœ¨å¿…è¦æ—¶å¼•ç”¨ä¿¡æ¯æ¥æºã€‚å¦‚æœå‘ç°ä¿¡æ¯å†²çªï¼Œè¯·æŒ‡å‡ºå¹¶è¯´æ˜åŸå› ã€‚"""

        # æ ¹æ®æ¨¡å‹é€‰æ‹©ä½¿ç”¨ä¸åŒçš„API
        if model_choice == "siliconflow":
            # ä½¿ç”¨SiliconFlow API
            result = call_siliconflow_api(prompt, temperature=0.7, max_tokens=1536)
            
            # å¤„ç†æ€ç»´é“¾
            processed_result = process_thinking_content(result)
            return processed_result
        else:
            # ä½¿ç”¨æœ¬åœ°Ollama
            response = session.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "deepseek-r1:7b",
                    "prompt": prompt,
                    "stream": False
                },
                timeout=120,
                headers={'Connection': 'close'}
            )
            response.raise_for_status()
            
            result = response.json()
            return process_thinking_content(str(result.get("response", "æœªè·å–åˆ°æœ‰æ•ˆå›ç­”")))
            
    except json.JSONDecodeError:
        return "å“åº”è§£æå¤±è´¥ï¼Œè¯·é‡è¯•"
    except KeyError:
        return "å“åº”æ ¼å¼å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æœåŠ¡"
    except Exception as e:
        return f"ç³»ç»Ÿé”™è¯¯: {str(e)}"

def process_thinking_content(text):
    """å¤„ç†åŒ…å«<think>æ ‡ç­¾çš„å†…å®¹ï¼Œå°†å…¶è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ–‡æœ¬
    if text is None:
        return ""
    
    # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
    if not isinstance(text, str):
        try:
            processed_text = str(text)
        except:
            return "æ— æ³•å¤„ç†çš„å†…å®¹æ ¼å¼"
    else:
        processed_text = text
    
    # å¤„ç†æ€ç»´é“¾æ ‡ç­¾
    try:
        while "<think>" in processed_text and "</think>" in processed_text:
            start_idx = processed_text.find("<think>")
            end_idx = processed_text.find("</think>")
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                thinking_content = processed_text[start_idx + 7:end_idx]
                before_think = processed_text[:start_idx]
                after_think = processed_text[end_idx + 8:]
                
                # ä½¿ç”¨å¯æŠ˜å è¯¦æƒ…æ¡†æ˜¾ç¤ºæ€ç»´é“¾
                processed_text = before_think + "\n\n<details>\n<summary>æ€è€ƒè¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n" + thinking_content + "\n\n</details>\n\n" + after_think
        
        # å¤„ç†å…¶ä»–HTMLæ ‡ç­¾ï¼Œä½†ä¿ç•™detailså’Œsummaryæ ‡ç­¾
        processed_html = []
        i = 0
        while i < len(processed_text):
            if processed_text[i:i+8] == "<details" or processed_text[i:i+9] == "</details" or \
               processed_text[i:i+8] == "<summary" or processed_text[i:i+9] == "</summary":
                # ä¿ç•™è¿™äº›æ ‡ç­¾
                tag_end = processed_text.find(">", i)
                if tag_end != -1:
                    processed_html.append(processed_text[i:tag_end+1])
                    i = tag_end + 1
                    continue
            
            if processed_text[i] == "<":
                processed_html.append("&lt;")
            elif processed_text[i] == ">":
                processed_html.append("&gt;")
            else:
                processed_html.append(processed_text[i])
            i += 1
        
        processed_text = "".join(processed_html)
    except Exception as e:
        logging.error(f"å¤„ç†æ€ç»´é“¾å†…å®¹æ—¶å‡ºé”™: {str(e)}")
        # å‡ºé”™æ—¶è‡³å°‘è¿”å›åŸå§‹æ–‡æœ¬ï¼Œä½†ç¡®ä¿å®‰å…¨å¤„ç†HTMLæ ‡ç­¾
        try:
            return text.replace("<", "&lt;").replace(">", "&gt;")
        except:
            return "å¤„ç†å†…å®¹æ—¶å‡ºé”™"
    
    return processed_text

def call_siliconflow_api(prompt, temperature=0.7, max_tokens=1024):
    """
    è°ƒç”¨SiliconFlow APIè·å–å›ç­”
    
    Args:
        prompt: æç¤ºè¯
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        
    Returns:
        ç”Ÿæˆçš„å›ç­”æ–‡æœ¬å’Œæ€ç»´é“¾å†…å®¹
    """
    print(f"prompt:{prompt}")
    # æ£€æŸ¥æ˜¯å¦é…ç½®äº†SiliconFlow APIå¯†é’¥
    if not SILICONFLOW_API_KEY:
        logging.error("æœªè®¾ç½® SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ API å¯†é’¥ã€‚")
        return "é”™è¯¯ï¼šæœªé…ç½® SiliconFlow API å¯†é’¥ã€‚", ""

    try:
        payload = {
            "model": "Qwen/Qwen2.5-VL-72B-Instruct",
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "stream": False,
            "max_tokens": max_tokens,
            "stop": None,
            "temperature": temperature,
            "top_p": 0.7,
            "top_k": 50,
            "frequency_penalty": 0.5,
            "n": 1,
            "response_format": {"type": "text"}
        }

        headers = {
            "Authorization": f"Bearer <ä½ çš„api>", # ä»ç¯å¢ƒå˜é‡è·å–å¯†é’¥
            "Content-Type": "application/json; charset=utf-8" # æ˜ç¡®æŒ‡å®šç¼–ç 
        }

        # æ‰‹åŠ¨å°†payloadç¼–ç ä¸ºUTF-8 JSONå­—ç¬¦ä¸²
        json_payload = json.dumps(payload, ensure_ascii=False).encode('utf-8')

        response = requests.post(
            SILICONFLOW_API_URL,
            data=json_payload, # é€šè¿‡dataå‚æ•°å‘é€ç¼–ç åçš„JSON
            headers=headers,
            timeout=60  # å»¶é•¿è¶…æ—¶æ—¶é—´
        )

        response.raise_for_status()
        result = response.json()
        
        # æå–å›ç­”å†…å®¹å’Œæ€ç»´é“¾
        if "choices" in result and len(result["choices"]) > 0:
            message = result["choices"][0]["message"]
            content = message.get("content", "")
            reasoning = message.get("reasoning_content", "")
            
            # å¦‚æœæœ‰æ€ç»´é“¾ï¼Œåˆ™æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œä»¥ä¾¿å‰ç«¯å¤„ç†
            if reasoning:
                # æ·»åŠ æ€ç»´é“¾æ ‡è®°
                full_response = f"{content}<think>{reasoning}</think>"
                return full_response
            else:
                print(f"content:{content}")
                return content
        else:
            return "APIè¿”å›ç»“æœæ ¼å¼å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥"
            
    except requests.exceptions.RequestException as e:
        logging.error(f"è°ƒç”¨SiliconFlow APIæ—¶å‡ºé”™: {str(e)}")
        return f"è°ƒç”¨APIæ—¶å‡ºé”™: {str(e)}"
    except json.JSONDecodeError:
        logging.error("SiliconFlow APIè¿”å›éJSONå“åº”")
        return "APIå“åº”è§£æå¤±è´¥"
    except Exception as e:
        logging.error(f"è°ƒç”¨SiliconFlow APIæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        return f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}"

from openai import OpenAI

# def call_siliconflow_api(prompt, temperature=0.7, max_tokens=1024, model="Pro/deepseek-ai/DeepSeek-R1"):
#     """
#     è°ƒç”¨SiliconFlow APIè·å–å›ç­”ï¼ˆéæµå¼å“åº”ï¼‰
    
#     Args:
#         prompt: æç¤ºè¯
#         temperature: æ¸©åº¦å‚æ•°
#         max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
#         model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        
#     Returns:
#         ç”Ÿæˆçš„å›ç­”æ–‡æœ¬ï¼ˆåŒ…å«å¯èƒ½çš„æ€ç»´é“¾å†…å®¹ï¼‰
#     """
#     # æ£€æŸ¥æ˜¯å¦é…ç½®äº†SiliconFlow APIå¯†é’¥
#     if not SILICONFLOW_API_KEY:
#         logging.error("æœªè®¾ç½® SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ API å¯†é’¥ã€‚")
#         return "é”™è¯¯ï¼šæœªé…ç½® SiliconFlow API å¯†é’¥ã€‚", ""

#     try:
#         # åˆ›å»ºOpenAIé£æ ¼çš„å®¢æˆ·ç«¯
#         client = OpenAI(
#             api_key=SILICONFLOW_API_KEY,
#             base_url=SILICONFLOW_API_URL
#         )
        
#         # åˆ›å»ºéæµå¼è¯·æ±‚
#         response = client.chat.completions.create(
#             model=model,
#             messages=[
#                 {"role": "user", "content": prompt}
#             ],
#             stream=False,  # éæµå¼å“åº”
#             max_tokens=max_tokens,
#             temperature=temperature,
#             top_p=0.7,
#             # top_k=50,
#             frequency_penalty=0.5
#         )
        
#         # å¤„ç†å“åº”
#         if response.choices and len(response.choices) > 0:
#             message = response.choices[0].message
#             content = message.content or ""
            
#             # å°è¯•è·å–æ€ç»´é“¾å†…å®¹ï¼ˆå¦‚æœAPIæ”¯æŒï¼‰
#             reasoning = ""
#             if hasattr(message, 'reasoning_content') and message.reasoning_content:
#                 reasoning = message.reasoning_content
            
#             # è¿”å›æ ¼å¼ï¼š(å›ç­”å†…å®¹, æ€ç»´é“¾å†…å®¹)
#             return content, reasoning
    
#     except Exception as e:
#         logging.error(f"è°ƒç”¨SiliconFlow APIæ—¶å‡ºé”™: {str(e)}")
#         return f"APIè°ƒç”¨é”™è¯¯: {str(e)}", ""
    
#     # é»˜è®¤è¿”å›ç©ºç»“æœ
#     return "", ""

def hybrid_merge(semantic_results, bm25_results, alpha=0.7):
    """
    åˆå¹¶è¯­ä¹‰æœç´¢å’ŒBM25æœç´¢ç»“æœ
    
    å‚æ•°:
        semantic_results: å‘é‡æ£€ç´¢ç»“æœ (å­—å…¸æ ¼å¼ï¼ŒåŒ…å«ids, documents, metadatas)
        bm25_results: BM25æ£€ç´¢ç»“æœ (å­—å…¸åˆ—è¡¨ï¼ŒåŒ…å«id, score, content)
        alpha: è¯­ä¹‰æœç´¢æƒé‡ (0-1)
        
    è¿”å›:
        åˆå¹¶åçš„ç»“æœåˆ—è¡¨ [(doc_id, {'score': score, 'content': content, 'metadata': metadata}), ...]
    """
    merged_dict = {}
    global faiss_metadatas_map # Ensure we can access the global map
    
    # å¤„ç†è¯­ä¹‰æœç´¢ç»“æœ
    if (semantic_results and 
        isinstance(semantic_results.get('documents'), list) and len(semantic_results['documents']) > 0 and
        isinstance(semantic_results.get('metadatas'), list) and len(semantic_results['metadatas']) > 0 and
        isinstance(semantic_results.get('ids'), list) and len(semantic_results['ids']) > 0 and
        isinstance(semantic_results['documents'][0], list) and 
        isinstance(semantic_results['metadatas'][0], list) and 
        isinstance(semantic_results['ids'][0], list) and
        len(semantic_results['documents'][0]) == len(semantic_results['metadatas'][0]) == len(semantic_results['ids'][0])):
        
        num_results = len(semantic_results['documents'][0])
        # Assuming semantic_results are already ordered by relevance (higher is better)
        # A simple rank-based score, can be replaced if actual scores/distances are available and preferred
        for i, (doc_id, doc, meta) in enumerate(zip(semantic_results['ids'][0], semantic_results['documents'][0], semantic_results['metadatas'][0])):
            score = 1.0 - (i / max(1, num_results)) # Higher rank (smaller i) gets higher score
            merged_dict[doc_id] = {
                'score': alpha * score, 
                'content': doc,
                'metadata': meta
            }
    else:
        logging.warning("Semantic results are missing, have an unexpected format, or are empty. Skipping semantic part in hybrid merge.")
    
    # å¤„ç†BM25ç»“æœ
    if not bm25_results:
        return sorted(merged_dict.items(), key=lambda x: x[1]['score'], reverse=True)
        
    valid_bm25_scores = [r['score'] for r in bm25_results if isinstance(r, dict) and 'score' in r]
    max_bm25_score = max(valid_bm25_scores) if valid_bm25_scores else 1.0
    
    for result in bm25_results:
        if not (isinstance(result, dict) and 'id' in result and 'score' in result and 'content' in result):
            logging.warning(f"Skipping invalid BM25 result item: {result}")
            continue
            
        doc_id = result['id']
        # Normalize BM25 score
        normalized_score = result['score'] / max_bm25_score if max_bm25_score > 0 else 0
        
        if doc_id in merged_dict:
            merged_dict[doc_id]['score'] += (1 - alpha) * normalized_score
        else:
            metadata = faiss_metadatas_map.get(doc_id, {}) # Get metadata from our global map
            merged_dict[doc_id] = {
                'score': (1 - alpha) * normalized_score,
                'content': result['content'],
                'metadata': metadata
            }
    
    merged_results = sorted(merged_dict.items(), key=lambda x: x[1]['score'], reverse=True)
    return merged_results

# æ–°å¢ï¼šæ›´æ–°æœ¬åœ°æ–‡æ¡£çš„BM25ç´¢å¼•
def update_bm25_index():
    """æ›´æ–°BM25ç´¢å¼•ï¼Œä»å†…å­˜ä¸­çš„æ˜ å°„åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
    global faiss_contents_map, faiss_id_order_for_index
    try:
        # Use the ordered list of IDs to ensure consistency
        doc_ids = faiss_id_order_for_index
        if not doc_ids:
            logging.warning("æ²¡æœ‰å¯ç´¢å¼•çš„æ–‡æ¡£ (FAISS IDåˆ—è¡¨ä¸ºç©º)")
            BM25_MANAGER.clear()
            return False

        # Retrieve documents in the correct order
        documents = [faiss_contents_map.get(doc_id, "") for doc_id in doc_ids]
        
        # Filter out any potential empty documents if necessary, though map access should be safe
        valid_docs_with_ids = [(doc_id, doc) for doc_id, doc in zip(doc_ids, documents) if doc]
        if not valid_docs_with_ids:
            logging.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹å¯ç”¨äºBM25ç´¢å¼•")
            BM25_MANAGER.clear()
            return False
            
        # Separate IDs and documents again for building the index
        final_doc_ids = [item[0] for item in valid_docs_with_ids]
        final_documents = [item[1] for item in valid_docs_with_ids]
            
        BM25_MANAGER.build_index(final_documents, final_doc_ids)
        logging.info(f"BM25ç´¢å¼•æ›´æ–°å®Œæˆï¼Œå…±ç´¢å¼• {len(final_doc_ids)} ä¸ªæ–‡æ¡£")
        return True
    except Exception as e:
        logging.error(f"æ›´æ–°BM25ç´¢å¼•å¤±è´¥: {str(e)}")
        return False

# æ–°å¢å‡½æ•°ï¼šè·å–ç³»ç»Ÿä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
def get_system_models_info():
    """è¿”å›ç³»ç»Ÿä½¿ç”¨çš„å„ç§æ¨¡å‹ä¿¡æ¯"""
    models_info = {
        "åµŒå…¥æ¨¡å‹": "all-MiniLM-L6-v2",
        "åˆ†å—æ–¹æ³•": "RecursiveCharacterTextSplitter (chunk_size=800, overlap=150)",
        "æ£€ç´¢æ–¹æ³•": "å‘é‡æ£€ç´¢ + BM25æ··åˆæ£€ç´¢ (Î±=0.7)",
        "é‡æ’åºæ¨¡å‹": "äº¤å‰ç¼–ç å™¨ (sentence-transformers/distiluse-base-multilingual-cased-v2)",
        "ç”Ÿæˆæ¨¡å‹": "deepseek-r1 (7B/1.5B)",
        "åˆ†è¯å·¥å…·": "jieba (ä¸­æ–‡åˆ†è¯)"
    }
    return models_info

# æ–°å¢å‡½æ•°ï¼šè·å–æ–‡æ¡£åˆ†å—å¯è§†åŒ–æ•°æ®
def get_document_chunks():
    """è·å–æ–‡æ¡£åˆ†å—ç»“æœç”¨äºå¯è§†åŒ–"""
    global faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index
    global chunk_data_cache # Ensure we can update the global cache
    try:
        # progress(0.1, desc="æ­£åœ¨ä»å†…å­˜åŠ è½½æ•°æ®...")
        
        if not faiss_id_order_for_index:
            chunk_data_cache = []
            return [], "çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£ï¼Œè¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£ã€‚"
            
        # progress(0.5, desc="æ­£åœ¨ç»„ç»‡åˆ†å—æ•°æ®...")
        
        doc_groups = {}
        # Iterate using the ordered IDs to reflect the FAISS index order conceptually
        for doc_id in faiss_id_order_for_index:
            doc = faiss_contents_map.get(doc_id, "")
            meta = faiss_metadatas_map.get(doc_id, {})
            if not doc: # Skip if content is somehow empty
                continue

            source = meta.get('source', 'æœªçŸ¥æ¥æº')
            if source not in doc_groups:
                doc_groups[source] = []
            
            doc_id_meta = meta.get('doc_id', 'æœªçŸ¥ID') # Get the original document ID from meta
            chunk_info = {
                "original_id": doc_id, # Keep the chunk-specific ID
                "doc_id": doc_id_meta,
                "content": doc[:200] + "..." if len(doc) > 200 else doc,
                "full_content": doc,
                "token_count": len(list(jieba.cut(doc))),
                "char_count": len(doc)
            }
            doc_groups[source].append(chunk_info)
        
        result_dicts = []
        result_lists = []
        
        # Keep track of chunks per source for indexing
        source_chunk_counters = {source: 0 for source in doc_groups.keys()}
        total_chunks = 0
        
        for source, chunks in doc_groups.items():
            num_chunks_in_source = len(chunks)
            for chunk in chunks:
                source_chunk_counters[source] += 1
                total_chunks += 1
                result_dict = {
                    "æ¥æº": source,
                    "åºå·": f"{source_chunk_counters[source]}/{num_chunks_in_source}",
                    "å­—ç¬¦æ•°": chunk["char_count"],
                    "åˆ†è¯æ•°": chunk["token_count"],
                    "å†…å®¹é¢„è§ˆ": chunk["content"],
                    "å®Œæ•´å†…å®¹": chunk["full_content"],
                    "åŸå§‹åˆ†å—ID": chunk["original_id"] # Add original ID for potential debugging
                }
                result_dicts.append(result_dict)
                
                result_lists.append([
                    source,
                    f"{source_chunk_counters[source]}/{num_chunks_in_source}",
                    chunk["char_count"],
                    chunk["token_count"],
                    chunk["content"]
                ])
        
        # progress(1.0, desc="æ•°æ®åŠ è½½å®Œæˆ!")
        
        chunk_data_cache = result_dicts # Update the global cache
        summary = f"æ€»è®¡ {total_chunks} ä¸ªæ–‡æœ¬å—ï¼Œæ¥è‡ª {len(doc_groups)} ä¸ªä¸åŒæ¥æºã€‚"
        
        return result_lists, summary
    except Exception as e:
        chunk_data_cache = []
        return [], f"è·å–åˆ†å—æ•°æ®å¤±è´¥: {str(e)}"

# æ·»åŠ å…¨å±€ç¼“å­˜å˜é‡
chunk_data_cache = []


# å®šä¹‰å‡½æ•°å¤„ç†äº‹ä»¶
def clear_chat_history():
    return None, "å¯¹è¯å·²æ¸…ç©º"

def process_chat(question, history, enable_web_search, model_choice):
    if history is None:
        history = []
    
    # æ›´æ–°æ¨¡å‹é€‰æ‹©ä¿¡æ¯çš„æ˜¾ç¤º
    api_text = """
    <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
        <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
        <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
        <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
    </div>
    """ % (
        "å·²å¯ç”¨" if enable_web_search else "æœªå¯ç”¨", 
        "Cloud DeepSeek-R1 æ¨¡å‹" if model_choice == "siliconflow" else "æœ¬åœ° Ollama æ¨¡å‹",
        "(éœ€è¦åœ¨.envæ–‡ä»¶ä¸­é…ç½®SERPAPI_KEY)" if enable_web_search else ""
    )
    
    # å¦‚æœé—®é¢˜ä¸ºç©ºï¼Œä¸å¤„ç†
    if not question or question.strip() == "":
        history.append(("", "é—®é¢˜ä¸èƒ½ä¸ºç©ºï¼Œè¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜ã€‚"))
        return history, "", api_text
    
    # æ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°å†å²
    history.append((question, ""))
    
    # åˆ›å»ºç”Ÿæˆå™¨
    resp_generator = stream_answer(question, enable_web_search, model_choice)
    
    # æµå¼æ›´æ–°å›ç­”
    for response, status in resp_generator:
        history[-1] = (question, response)
        yield history, "", api_text

def process_chat(question, history, enable_web_search, model_choice):
    if history is None:
        history = []
    
    api_text = """
    <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
        <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
        <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
        <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
    </div>
    """ % (
        "å·²å¯ç”¨" if enable_web_search else "æœªå¯ç”¨", 
        "Cloud DeepSeek-R1 æ¨¡å‹" if model_choice == "siliconflow" else "æœ¬åœ° Ollama æ¨¡å‹",
        "(éœ€è¦åœ¨.envæ–‡ä»¶ä¸­é…ç½®SERPAPI_KEY)" if enable_web_search else ""
    )
    
    if not question or question.strip() == "":
        history.append(("", "é—®é¢˜ä¸èƒ½ä¸ºç©ºï¼Œè¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜ã€‚"))
        return history, "", api_text
    
    history.append((question, ""))
    
    # è¿™é‡Œç”¨åˆ—è¡¨æ”¶é›†æµå¼è¿”å›çš„å†…å®¹
    full_response = ""
    resp_generator = stream_answer(question, enable_web_search, model_choice)
    for response, status in resp_generator:
        full_response = response
    
    # æ›´æ–°å†å²å›ç­”ä¸ºå®Œæ•´ç»“æœ
    history[-1] = (question, full_response)
    
    return history, "", api_text


def update_api_info(enable_web_search, model_choice):
    api_text = """
    <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
        <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
        <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
        <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
    </div>
    """ % (
        "å·²å¯ç”¨" if enable_web_search else "æœªå¯ç”¨", 
        "Cloud DeepSeek-R1 æ¨¡å‹" if model_choice == "siliconflow" else "æœ¬åœ° Ollama æ¨¡å‹",
        "(éœ€è¦åœ¨.envæ–‡ä»¶ä¸­é…ç½®SERPAPI_KEY)" if enable_web_search else ""
    )
    return api_text

    # ç»‘å®šUIäº‹ä»¶
    upload_btn.click(
        process_multiple_pdfs,
        inputs=[file_input],
        outputs=[upload_status, file_list],
        show_progress=True
    )

    # ç»‘å®šæé—®æŒ‰é’®
    ask_btn.click(
        process_chat,
        inputs=[question_input, chatbot, web_search_checkbox, model_choice],
        outputs=[chatbot, question_input, api_info]
    )

    # ç»‘å®šæ¸…ç©ºæŒ‰é’®
    clear_btn.click(
        clear_chat_history,
        inputs=[],
        outputs=[chatbot, status_display]
    )

    # å½“åˆ‡æ¢è”ç½‘æœç´¢æˆ–æ¨¡å‹é€‰æ‹©æ—¶æ›´æ–°APIä¿¡æ¯
    web_search_checkbox.change(
        update_api_info,
        inputs=[web_search_checkbox, model_choice],
        outputs=[api_info]
    )
    
    model_choice.change(
        update_api_info,
        inputs=[web_search_checkbox, model_choice],
        outputs=[api_info]
    )
    
    # æ–°å¢ï¼šåˆ†å—å¯è§†åŒ–åˆ·æ–°æŒ‰é’®äº‹ä»¶
    refresh_chunks_btn.click(
        fn=get_document_chunks,
        outputs=[chunks_data, chunks_status]
    )
    
    # æ–°å¢ï¼šåˆ†å—è¡¨æ ¼ç‚¹å‡»äº‹ä»¶
    chunks_data.select(
        fn=show_chunk_details,
        inputs=chunks_data,
        outputs=chunk_detail_text
    )




# æ¢å¤ä¸»ç¨‹åºå¯åŠ¨éƒ¨åˆ†
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æœ¬åœ°RAGé—®ç­”ç³»ç»Ÿ")
    parser.add_argument("--files", nargs="+", help="PDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨")
    parser.add_argument("--web-search", action="store_true", help="å¯ç”¨ç½‘ç»œæœç´¢")
    parser.add_argument("--model", choices=["ollama", "siliconflow"], default="ollama", help="é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹")
    
    args = parser.parse_args()
    if args.files:
        process_multiple_pdfs(args.files)

    process_chat("è¯·å¸®æˆ‘æ‰¾åˆ°å›½å®¶ç¯å¢ƒä¿æŠ¤å·¥ä¸šåºŸæ°´æ±¡æŸ“æ§åˆ¶å·¥ç¨‹æŠ€æœ¯ä¸­å¿ƒè´Ÿè´£äººçš„é‚®ç®±", None , False,"siliconflow")
    # if not check_environment():
    #     print("ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿OllamaæœåŠ¡å·²å¯åŠ¨")
    #     exit(1)
    
    # if args.files:
    #     print("å¼€å§‹å¤„ç†æ–‡æ¡£...")
    #     result, _ = process_documents(args.files)
    #     print(result)
    
    # print("\nå¼€å§‹äº¤äº’å¼é—®ç­”...")
    # interactive_qa(args.web_search, args.model)

